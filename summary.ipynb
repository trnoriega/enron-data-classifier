{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Enron Data. Based on Udacity intro to machine learning course\n",
    "\n",
    "email data can be found at: https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tgz\n",
    "and 'data/emails_by_address/'\n",
    "financial data can be found in 'financial_data.pdf' in this repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Suppress package warnings, most of which are caused by deprecations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the dictionary containing financial data and some email features (to_ from_poi)\n",
    "import pickle\n",
    "with open('data/final_project_dataset.pkl', 'rb') as f:\n",
    "    fin_data_dict = pickle.load(f)\n",
    "\n",
    "# Remove outliers\n",
    "fin_data_dict.pop('TOTAL', 0)\n",
    "\n",
    "# Store to my_dataset for easy export below.\n",
    "my_dataset = fin_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select features to use\n",
    "\n",
    "# features_list is a list of feature names in the financial data.\n",
    "# The first feature must be \"poi\".\n",
    "\n",
    "import numpy as np\n",
    "from tools.feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "def make_features_labels(dataset, feature_names):\n",
    "    \"\"\"\n",
    "    Quick way to split a dataset into features and labels based on feature names\n",
    "    \"\"\"\n",
    "    data = featureFormat(dataset, feature_names, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    labels = np.array(labels)\n",
    "    features = np.array(features)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# Start with all features except: 'email_address'\n",
    "\n",
    "all_feature_names = ['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments',\\\n",
    "'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi',\\\n",
    "'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances',\\\n",
    "'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income',\\\n",
    "'long_term_incentive', 'from_poi_to_this_person']\n",
    "\n",
    "all_features, all_labels = make_features_labels(my_dataset, all_feature_names)\n",
    "\n",
    "# Select the most important features based on ExtraTreesClassifier\n",
    "from feature_selection import importance_plotter\n",
    "selected_feature_names = importance_plotter(all_features, all_labels, \n",
    "                                            np.array(all_feature_names[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Confirm that the feature selection did help performance:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "\n",
    "# Make features and labels based on the new selection\n",
    "selected_feature_names.insert(0, 'poi')\n",
    "selected_features, selected_labels = make_features_labels(my_dataset, selected_feature_names)\n",
    "\n",
    "clf = GaussianNB()\n",
    "def fit_print_scores(clf, features, labels):\n",
    "    \n",
    "    cv = StratifiedShuffleSplit(test_size=0.3, random_state=42)\n",
    "    f1_scores = cross_val_score(clf, features, labels, cv=5, scoring='f1')\n",
    "    print '-f1 score: %0.2f (+/- %0.2f)' % (f1_scores.mean(), \n",
    "                                           f1_scores.std() * 2)\n",
    "    precision_scores = cross_val_score(clf, features, labels, cv=5, scoring='precision')\n",
    "    print '-precision score: %0.2f (+/- %0.2f)' % (precision_scores.mean(), \n",
    "                                                  precision_scores.std() * 2)\n",
    "    recall_scores = cross_val_score(clf, features, labels, cv=5, scoring='recall')\n",
    "    print '-recall score: %0.2f (+/- %0.2f)' % (recall_scores.mean(), \n",
    "                                                  recall_scores.std() * 2)\n",
    "\n",
    "print 'All features:'\n",
    "fit_print_scores(clf, all_features, all_labels)\n",
    "print '\\nSelected features:'\n",
    "fit_print_scores(clf, selected_features, selected_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this feature selection approach does not really improve performance, especially since the recall score suffers a lot. \n",
    "\n",
    "Try again, but this time selecting features by training the decision trees that determine feature importance on a data set with equal amounts of pois and non-pois. This should address imbalanced trees that are biased towards non-pois. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from balanced_trees import data_balancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_bal_features, _, train_bal_labels, _ = train_test_split(all_features, all_labels, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "balanced_features, balanced_labels = data_balancer(train_bal_features, train_bal_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "balanced_selected_feature_names = importance_plotter(balanced_features, balanced_labels, \n",
    "                                                     np.array(all_feature_names[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_selected_feature_names.insert(0, 'poi')\n",
    "balanced_selected_features, balanced_selected_labels = \\\n",
    "    make_features_labels(my_dataset, balanced_selected_feature_names)\n",
    "    \n",
    "print 'All features:'\n",
    "fit_print_scores(clf, all_features, all_labels)\n",
    "print '\\nSelected features after balancing:'\n",
    "fit_print_scores(clf, balanced_selected_features, balanced_selected_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems better, at least the recal score doesn't suffer as much and the f1 and precision scores are still decent.The confidence intervals have also gotten smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the most highly correlated features to see if there is any redundancy or outliers \n",
    "from feature_selection import correlation_plotter\n",
    "correlation_plotter(balanced_selected_feature_names[1:], my_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation graphs it seems like there is not much redundancy in the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create new features\n",
    "# word_dict_subset is a dictionary that contains emails from all of the people\n",
    "# with financial data AND email data available separated into 'to' and 'from categories. \n",
    "# 86 people total, 14 pois\n",
    "\n",
    "import pickle\n",
    "with open('data/word_dict_subset.pkl', 'rb') as f:\n",
    "    data_text = pickle.load(f)\n",
    "\n",
    "#Separate data into emails from, to, or all-together\n",
    "from tools.email_analysis import email_list_and_labels\n",
    "to_emails, to_email_labels = email_list_and_labels(data_text, 'to')\n",
    "from_emails, from_email_labels = email_list_and_labels(data_text, 'from')\n",
    "all_emails, all_email_labels = email_list_and_labels(data_text, 'all')\n",
    "\n",
    "# Create a tf-idf vectors of each person's to, from, and all-together emails\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5)\n",
    "\n",
    "to_feature_matrix = vectorizer.fit_transform(to_emails)\n",
    "from_feature_matrix = vectorizer.fit_transform(from_emails)\n",
    "all_feature_matrix = vectorizer.fit_transform(all_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "print 'To emails scores:'\n",
    "fit_print_scores(clf, to_feature_matrix.toarray(), to_email_labels)\n",
    "print '\\nFrom emails scores:'\n",
    "fit_print_scores(clf, from_feature_matrix.toarray(), from_email_labels)\n",
    "print '\\nAll emails scores:'\n",
    "fit_print_scores(clf, all_feature_matrix.toarray(), all_email_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like email word extraction does not yield very promising results, will drop for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try a variety of classifiers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# for ease rename balanced_selected_features and balanced_selected_labels\n",
    "features = balanced_selected_features\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "labels = balanced_selected_labels\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "print 'Gaussian naive Bayes:'\n",
    "fit_print_scores(clf, features, labels)\n",
    "\n",
    "from sklearn.svm import SVC \n",
    "clf = SVC()\n",
    "print '\\nSupport vector machine:'\n",
    "fit_print_scores(clf, scaled_features, labels)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "clf = RandomForestClassifier()\n",
    "print '\\nRandom forest:'\n",
    "fit_print_scores(clf, features, labels)\n",
    "\n",
    "clf = ExtraTreesClassifier()\n",
    "print '\\nExtra trees:'\n",
    "fit_print_scores(clf, features, labels)\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "print '\\nAdaBoost:'\n",
    "fit_print_scores(clf, features, labels)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "print('\\nLogistic regression:')\n",
    "fit_print_scores(clf, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most promising classifiers are:\n",
    "AdaBoost, Naive Bayes, and Random Forest. Will tune parameters for these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tune classifier to achieve better than .3 precision and recall \n",
    "train_features, test_features, train_labels, test_labels =\\\n",
    "train_test_split(features, labels, test_size = 0.4, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print \"# Tuning hyper-parameters for %s\\n\" % score\n",
    "    cv = StratifiedShuffleSplit(test_size=0.3, random_state=1)\n",
    "    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=cv,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(train_features, train_labels)\n",
    "\n",
    "    print 'Best parameters set found on development set:'\n",
    "    print clf.best_params_\n",
    "    print '\\n'\n",
    "    print 'Grid scores on development set:'\n",
    "    print '\\n'\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print '%0.3f (+/-%0.03f) for %r' % (mean, std * 2, params)\n",
    "    print '\\n'\n",
    "\n",
    "    print 'Detailed classification report:'\n",
    "    print '\\n'\n",
    "    \n",
    "    y_true, y_pred = test_labels, clf.predict(test_features)\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "from tools.tester import dump_classifier_and_data\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
