{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Enron Data\n",
    "## Based on Udacity intro to machine learning course\n",
    "\n",
    "### Data sources:\n",
    "- Raw email text data can be found at: https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tgz\n",
    "and a breakdown of emails by sender can be found [here](data/emails_by_address/)\n",
    "- The financial data was compiled from [this file](data/financial_data.pdf)\n",
    "- The persons of interest \"pois\" come from [this file](data/poi_names.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Support functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tools.feature_format import featureFormat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data already preprocessed in [outlier_removal](outlier_removal.ipynb) and [imputing_data](imputing_data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('imputer_dicts.pkl', 'rb') as f:\n",
    "    imputer_dicts = pickle.load(f)\n",
    "\n",
    "mean_data_dict = imputer_dicts['mean']\n",
    "\n",
    "# Start with all features except: 'email_address'\n",
    "all_feature_names = ['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments',\\\n",
    "'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi',\\\n",
    "'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances',\\\n",
    "'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income',\\\n",
    "'long_term_incentive', 'from_poi_to_this_person']\n",
    "\n",
    "mean_data = featureFormat(mean_data_dict, all_feature_names)\n",
    "labels, pre_features = mean_data[:,0], mean_data[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "                 ('scaler', StandardScaler()),\n",
    "                 ('polynomials_addr', PolynomialFeatures(2)),\n",
    "                 ('feature_selr', SelectFromModel(\n",
    "                                                 ExtraTreesClassifier(\n",
    "                                                     random_state=2,\n",
    "                                                     class_weight='balanced'), \n",
    "                                                 threshold='mean'))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-f1 score: 0.33 (+/- 0.68)\n",
      "-precision score: 0.36 (+/- 0.78)\n",
      "-recall score: 0.31 (+/- 0.62)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "def fit_print_scores(clf, features, labels):\n",
    "    \n",
    "    f1_scores = cross_val_score(clf, features, labels, cv=6, scoring='f1')\n",
    "    print '-f1 score: %0.2f (+/- %0.2f)' % (f1_scores.mean(), \n",
    "                                           f1_scores.std() * 2)\n",
    "    precision_scores = cross_val_score(clf, features, labels, cv=6, scoring='precision')\n",
    "    print '-precision score: %0.2f (+/- %0.2f)' % (precision_scores.mean(), \n",
    "                                                  precision_scores.std() * 2)\n",
    "    recall_scores = cross_val_score(clf, features, labels, cv=6, scoring='recall')\n",
    "    print '-recall score: %0.2f (+/- %0.2f)' % (recall_scores.mean(), \n",
    "                                                  recall_scores.std() * 2)\n",
    "\n",
    "features = pipe.fit_transform(pre_features, labels)\n",
    "\n",
    "fit_print_scores(GaussianNB(), features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian naive Bayes:\n",
      "-f1 score: 0.33 (+/- 0.68)\n",
      "-precision score: 0.36 (+/- 0.78)\n",
      "-recall score: 0.31 (+/- 0.62)\n",
      "\n",
      "Support vector machine:\n",
      "-f1 score: 0.00 (+/- 0.00)\n",
      "-precision score: 0.00 (+/- 0.00)\n",
      "-recall score: 0.00 (+/- 0.00)\n",
      "\n",
      "Extra trees:\n",
      "-f1 score: 0.19 (+/- 0.56)\n",
      "-precision score: 0.25 (+/- 0.76)\n",
      "-recall score: 0.17 (+/- 0.33)\n",
      "\n",
      "AdaBoost:\n",
      "-f1 score: 0.06 (+/- 0.25)\n",
      "-precision score: 0.06 (+/- 0.25)\n",
      "-recall score: 0.06 (+/- 0.25)\n",
      "\n",
      "Logistic regression:\n",
      "-f1 score: 0.28 (+/- 0.57)\n",
      "-precision score: 0.44 (+/- 0.92)\n",
      "-recall score: 0.22 (+/- 0.50)\n"
     ]
    }
   ],
   "source": [
    "### Try a variety of classifiers\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "print 'Gaussian naive Bayes:'\n",
    "fit_print_scores(clf, features, labels)\n",
    "\n",
    "from sklearn.svm import SVC \n",
    "clf = SVC()\n",
    "print '\\nSupport vector machine:'\n",
    "fit_print_scores(clf, features, labels)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "\n",
    "clf = ExtraTreesClassifier()\n",
    "print '\\nExtra trees:'\n",
    "fit_print_scores(clf, features, labels)\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "print '\\nAdaBoost:'\n",
    "fit_print_scores(clf, features, labels)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "print('\\nLogistic regression:')\n",
    "fit_print_scores(clf, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most promising classifiers are:\n",
    "AdaBoost, Naive Bayes, ExtraTrees, and logistic regression. Will tune parameters for AdaBoost and ExtraTrees and logistic regression. Naive Bayes does not have parameters to tune. So will test on the same data as the other three to have a direct comparison.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.94      0.90      0.92        51\n",
      "        1.0       0.38      0.50      0.43         6\n",
      "\n",
      "avg / total       0.88      0.86      0.87        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features, train_labels, test_labels =\\\n",
    "train_test_split(features, labels, test_size = 0.4, random_state=0)\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(train_features, train_labels)\n",
    "y_true, y_pred = test_labels, clf.predict(test_features)\n",
    "print classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_keys = []\n",
    "end_str = '_test_score'\n",
    "start_str = 'split'\n",
    "for num in range(5):\n",
    "    complete_str = start_str+str(num)+end_str\n",
    "    score_keys.append(complete_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 460 tasks      | elapsed:   45.7s\n",
      "[Parallel(n_jobs=-1)]: Done 639 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1890 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2400 out of 2400 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'n_estimators': 100, 'learning_rate': 0.1, 'base_estimator__max_depth': 3, 'base_estimator__min_samples_leaf': 8}\n",
      "\n",
      "Detailed scores:\n",
      "0.43 +/-(0.25)\n",
      "[0.66666666666666663, 0.5, 0.0, 0.33333333333333331, 0.66666666666666663]\n",
      "\n",
      "Scores on test set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.93      0.98      0.95        51\n",
      "        1.0       0.67      0.33      0.44         6\n",
      "\n",
      "avg / total       0.90      0.91      0.90        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Tune classifiers to achieve better than .3 precision and recall \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tuned_parameters = {'base_estimator__max_depth' : [None, 3, 6, 12, 24, 48],\n",
    "                    'base_estimator__min_samples_leaf': [1, 2, 4, 8, 16],\n",
    "                    'n_estimators': [25, 50, 100, 250],\n",
    "                    'learning_rate': [0.01, 0.1, 1, 10]}\n",
    "\n",
    "scores = ['f1']\n",
    "\n",
    "DTC = DecisionTreeClassifier(random_state=1, class_weight='balanced')\n",
    "\n",
    "ABC = AdaBoostClassifier(base_estimator=DTC, random_state=2)\n",
    "\n",
    "print \"# Tuning hyper-parameters for %s\\n\" % score\n",
    "ABC_search = GridSearchCV(ABC, param_grid=tuned_parameters, cv=5, \n",
    "                   verbose=1, n_jobs=-1,\n",
    "                   scoring='%s' % score)\n",
    "ABC_search.fit(train_features, train_labels)\n",
    "\n",
    "print 'Best parameters set found on development set:'\n",
    "print ABC_search.best_params_\n",
    "print\n",
    "\n",
    "cv_results = ABC_search.cv_results_\n",
    "best_test_scores = []\n",
    "best_idx = ABC_search.best_index_\n",
    "for key in score_keys:\n",
    "    best_test_scores.append(cv_results[key][best_idx])\n",
    "\n",
    "best_mean, best_std = np.mean(best_test_scores), np.std(best_test_scores)\n",
    "print 'Detailed scores:'\n",
    "print '%0.2f +/-(%0.2f)' %(best_mean, best_std)\n",
    "print best_test_scores\n",
    "print\n",
    "\n",
    "print 'Scores on test set:'\n",
    "y_true, y_pred = test_labels, ABC_search.predict(test_features)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   31.3s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'max_features': 1, 'n_estimators': 100, 'max_depth': 3, 'min_samples_leaf': 4}\n",
      "\n",
      "Detailed scores:\n",
      "0.54 +/-(0.34)\n",
      "[0.5, 0.80000000000000004, 0.40000000000000002, 0.0, 1.0]\n",
      "\n",
      "Scores on test set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.90      0.86      0.88        51\n",
      "        1.0       0.12      0.17      0.14         6\n",
      "\n",
      "avg / total       0.82      0.79      0.80        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = {'max_depth' : [None, 3, 6, 12, 24, 48],\n",
    "                    'min_samples_leaf': [1, 2, 4, 8],\n",
    "                    'n_estimators': [25, 50, 100],\n",
    "                    'max_features': [1, 2, 4, 6, 8, 10]}\n",
    "\n",
    "scores = ['f1']\n",
    "\n",
    "ETC = ExtraTreesClassifier(class_weight='balanced')\n",
    "\n",
    "print \"# Tuning hyper-parameters for %s\\n\" % score\n",
    "ETC_search = GridSearchCV(ETC, param_grid=tuned_parameters, cv=5, \n",
    "                   verbose=1, n_jobs=-1,\n",
    "                   scoring='f1')\n",
    "ETC_search.fit(train_features, train_labels)\n",
    "\n",
    "print 'Best parameters set found on development set:'\n",
    "print ETC_search.best_params_\n",
    "print\n",
    "\n",
    "cv_results = ETC_search.cv_results_\n",
    "best_test_scores = []\n",
    "best_idx = ETC_search.best_index_\n",
    "for key in score_keys:\n",
    "    best_test_scores.append(cv_results[key][best_idx])\n",
    "\n",
    "best_mean, best_std = np.mean(best_test_scores), np.std(best_test_scores)\n",
    "print 'Detailed scores:'\n",
    "print '%0.2f +/-(%0.2f)' %(best_mean, best_std)\n",
    "print best_test_scores\n",
    "print\n",
    "\n",
    "print 'Scores on test set:'\n",
    "y_true, y_pred = test_labels, ETC_search.predict(test_features)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
      "Best parameters set found on development set:\n",
      "{'warm_start': False, 'C': 0.1, 'max_iter': 100, 'tol': 0.0001, 'class_weight': None}\n",
      "\n",
      "Detailed scores:\n",
      "0.33 +/-(0.28)\n",
      "[0.0, 0.5, 0.5, 0.0, 0.66666666666666663]\n",
      "\n",
      "Scores on test set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      0.98      0.93        51\n",
      "        1.0       0.00      0.00      0.00         6\n",
      "\n",
      "avg / total       0.80      0.88      0.84        57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    3.5s finished\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = {\n",
    "                    'C': [1e-2, 1e-1, 1, 1e1, 1e2],\n",
    "                    'class_weight': [None, 'balanced'],\n",
    "                    'max_iter': [100, 200],\n",
    "                    'tol': [1e-4, 5e-4, 1e-3],\n",
    "                    'warm_start': [False, True]\n",
    "                    }\n",
    "\n",
    "LR = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "LR_search = GridSearchCV(LR, param_grid=tuned_parameters, cv=5, \n",
    "                   verbose=1, n_jobs=-1,\n",
    "                   scoring='f1')\n",
    "\n",
    "LR_search.fit(train_features, train_labels)\n",
    "\n",
    "print 'Best parameters set found on development set:'\n",
    "print LR_search.best_params_\n",
    "print \n",
    "\n",
    "cv_results = LR_search.cv_results_\n",
    "best_test_scores = []\n",
    "best_idx = LR_search.best_index_\n",
    "for key in score_keys:\n",
    "    best_test_scores.append(cv_results[key][best_idx])\n",
    "\n",
    "best_mean, best_std = np.mean(best_test_scores), np.std(best_test_scores)\n",
    "print 'Detailed scores:'\n",
    "print '%0.2f +/-(%0.2f)' %(best_mean, best_std)\n",
    "print best_test_scores\n",
    "print\n",
    "\n",
    "print 'Scores on test set:'\n",
    "y_true, y_pred = test_labels, LR_search.predict(test_features)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Dump classifier, dataset, and features_list so anyone can check your results.\n",
    "\n",
    "from tools.tester import dump_classifier_and_data\n",
    "my_dataset = fin_data\n",
    "features_list = selected_feature_names\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
