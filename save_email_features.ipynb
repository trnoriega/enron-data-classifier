{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the email features created with TfIdf to a dict that can be merged with the financial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tuple list created in [vectorize_email_features](vectorize_email_features.ipynb) as `label_email_text` and the original project data as `data_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "HOME_PATH = os.path.expanduser('~')\n",
    "DATA_PATH = os.path.join(HOME_PATH, 'Desktop', 'raw_data', 'ml')\n",
    "\n",
    "label_email_text_path = os.path.join(DATA_PATH, 'label_email_text.pkl')\n",
    "with open(label_email_text_path, 'rb') as f:\n",
    "    label_email_text = pickle.load(f)\n",
    "\n",
    "with open('data/final_project_dataset.pkl', 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "del data_dict['TOTAL']\n",
    "del data_dict['LOCKHART EUGENE E']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate emails into to, from, or all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 90, 90, 90)\n"
     ]
    }
   ],
   "source": [
    "labels, to_text, from_text, all_text, emails = zip(*label_email_text)\n",
    "print(len(to_text), len(from_text), len(all_text), len(emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "\n",
    "from_array = vectorizer.fit_transform(from_text)\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "sel_from_array = selector.fit_transform(from_array, labels)\n",
    "sel_idxs = selector.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list with the selected words used as keys in the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab dictionary before selection:  42115 compare to:  42115\n",
      "Vocab dictionary AFTER selection:  4212 compare to:  4212\n",
      "Length of list with selected words: 4212 compare with: 4212\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.vocabulary_\n",
    "\n",
    "print 'Vocab dictionary before selection: ', len(vocab),\\\n",
    "      'compare to: ', from_array.shape[1] \n",
    "sel_vocab = {key: item for key, item in vocab.items() if item in sel_idxs}\n",
    "\n",
    "print 'Vocab dictionary AFTER selection: ', len(sel_vocab),\\\n",
    "      'compare to: ', sel_from_array.shape[1]\n",
    "\n",
    "vocab_list = [[key, idx] for key, idx in vocab.items()]\n",
    "keys, idxs = zip(*vocab_list)\n",
    "keys, idxs = np.array(keys), np.array(idxs)\n",
    "keys = keys[np.argsort(idxs)]\n",
    "sel_vocab_list = keys[selector.get_support()]\n",
    "\n",
    "print 'Length of list with selected words:',\\\n",
    "      len(sel_vocab_list), 'compare with:', sel_from_array.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out what names from the original dataset need to be added to `sel_from_array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_to_emails =\\\n",
    "[(key, item['email_address']) for key, item in data_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "# Out of those 144 names, how many are in the from_array?\n",
    "in_vector = [tup[1] for tup in names_to_emails if tup[1] in emails]\n",
    "print len(in_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m..forney@enron.com', 'tim.despain@enron.com', 'larry.lawyer@enron.com', 'jeff.richter@enron.com']\n"
     ]
    }
   ],
   "source": [
    "# There are 4 emails missing, find out what they are\n",
    "out = [email for email in emails if email not in in_vector]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are they pois?\n",
    "[tup[0] for tup in label_email_text if tup[4] in out]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All values not in original dataset are POIs so I will include them in new dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "vector_only_names_emails = (('FORNEY M', 'm..forney@enron.com'),\n",
    "                            ('DESPAIN TIM', 'tim.despain@enron.com'),\n",
    "                            ('LAWYER LARRY', 'larry.lawyer@enron.com'),\n",
    "                            ('RICHTER JEFF', 'jeff.richter@enron.com'))\n",
    "print(len(names_to_emails))\n",
    "names_to_emails.extend(vector_only_names_emails)\n",
    "print(len(names_to_emails))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists with all the names, labels, and emails in the vector array as well as the missing ones from the original `data_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 58 compare to:  58\n"
     ]
    }
   ],
   "source": [
    "data_only_names_emails = [tup for tup in names_to_emails if tup[1] not in emails]\n",
    "print len(data_only_names_emails), 'compare to: ', len(data_dict) - len(in_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 compare to:  148\n"
     ]
    }
   ],
   "source": [
    "data_names, data_emails =zip(*data_only_names_emails)\n",
    "all_emails = list(emails) + list(data_emails)\n",
    "print len(all_emails), 'compare to: ', len(names_to_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vector_names: 90 \n",
      "last vector name and email: MCCARTY DANNY J danny.mccarty@enron.com \n",
      "length all names: 148 compare to: 148\n"
     ]
    }
   ],
   "source": [
    "vector_names = []\n",
    "for email in emails:\n",
    "    vector_names.extend([tup[0] for tup in names_to_emails if tup[1] == email])\n",
    "all_names = vector_names + list(data_names)\n",
    "\n",
    "print 'length of vector_names:', len(vector_names),\\\n",
    "      '\\nlast vector name and email:', vector_names[89], emails [89],\\\n",
    "      '\\nlength all names:', len(all_names), 'compare to:', len(all_emails)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 compare to: 148\n"
     ]
    }
   ],
   "source": [
    "data_only_labels = [data_dict[name]['poi'] for name in data_names]\n",
    "all_labels = list(labels) + data_only_labels\n",
    "print len(all_labels), 'compare to:', len(all_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the missing `data_dict` samples and impute values to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original array shape: (90, 4212) \n",
      "array to append shape: (58, 4212)\n"
     ]
    }
   ],
   "source": [
    "to_append = np.zeros((len(data_emails), sel_from_array.shape[1]))\n",
    "to_append.fill(np.nan)\n",
    "print 'original array shape:', sel_from_array.shape,\\\n",
    "      '\\narray to append shape:', to_append.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of merged array: (148, 4212)\n"
     ]
    }
   ],
   "source": [
    "new_array = np.concatenate((sel_from_array.toarray(), to_append), axis=0)\n",
    "print 'Shape of merged array:', new_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of array with imputed values: (148, 4212) compare with: (148, 4212)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(strategy='most_frequent')\n",
    "imp_new_array = imp.fit_transform(new_array)\n",
    "print 'Shape of array with imputed values:', imp_new_array.shape,\\\n",
    "      'compare with:', new_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.92      1.00      0.96        33\n",
      "       True       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.84      0.92      0.88        36\n",
      "\n",
      "Imputed with additions: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.90      1.00      0.95        53\n",
      "       True       1.00      0.14      0.25         7\n",
      "\n",
      "avg / total       0.91      0.90      0.87        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rdm = 200\n",
    "\n",
    "train_features_sel, test_features_sel, train_labels_sel, test_labels_sel =\\\n",
    "train_test_split(sel_from_array.toarray(), labels, test_size=0.4, random_state=rdm)\n",
    "\n",
    "train_features, test_features, train_labels, test_labels =\\\n",
    "train_test_split(imp_new_array, all_labels, test_size=0.4, random_state=rdm)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print 'Original: '\n",
    "clf = GaussianNB()\n",
    "clf.fit(train_features_sel, train_labels_sel)\n",
    "pred = clf.predict(test_features_sel)\n",
    "print(classification_report(test_labels_sel, pred))\n",
    "\n",
    "print 'Imputed with additions: '\n",
    "clf = GaussianNB()\n",
    "clf.fit(train_features, train_labels)\n",
    "pred = clf.predict(test_features)\n",
    "print(classification_report(test_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing does not seem to hurt the scores\n",
    "\n",
    "\n",
    "Create the dictionary from the merged array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dict = {}\n",
    "for ie, email in enumerate(all_emails):\n",
    "    for tup in names_to_emails:\n",
    "        if tup[1] == email:\n",
    "            name = tup[0]\n",
    "            vector_dict[name]  = {}\n",
    "            vector_dict[name]['poi'] = all_labels[ie]\n",
    "            for iw, word in enumerate(sel_vocab_list):\n",
    "                vector_dict[name][word] = float(imp_new_array[ie,iw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All internal keys add up? True\n"
     ]
    }
   ],
   "source": [
    "for key, item in vector_dict.items():\n",
    "    good = True\n",
    "    if len(item) != 4213:\n",
    "        good = False\n",
    "print 'All internal keys add up?', good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that when imported back the dictionary data is the same as the source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 4213\n",
      "(148,) (148,) (148, 4212)\n"
     ]
    }
   ],
   "source": [
    "from tools.feature_format import featureFormat\n",
    "\n",
    "sel_vocab_list = list(sel_vocab_list)\n",
    "sel_vocab_list.insert(0, 'poi')\n",
    "\n",
    "print len(vector_dict), len(sel_vocab_list)\n",
    "data = featureFormat(vector_dict, sel_vocab_list,\n",
    "                     remove_NaN=True, keep_keys=True, remove_all_zeroes=False)\n",
    "keys, labels, features = data[:,0], data[:,1].astype(float), data[:,2:].astype(float)\n",
    "print keys.shape, labels.shape, features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names equal: True\n",
      "labels equal: True\n",
      "features equal: False\n"
     ]
    }
   ],
   "source": [
    "# Organize the pre_dict data and post_dict data the same way so that they can be compared\n",
    "\n",
    "all_names = np.array(all_names)\n",
    "all_to_sort = np.argsort(all_names)\n",
    "all_to_sort\n",
    "sort_all_names = all_names[all_to_sort]\n",
    "sort_all_labels = np.array(all_labels).astype(np.int8)[all_to_sort]\n",
    "sort_imp_new_array = imp_new_array.astype(np.float32)[all_to_sort,:] \n",
    "\n",
    "new_to_sort = np.argsort(keys)\n",
    "sort_keys = keys[new_to_sort]\n",
    "sort_labels = labels.astype(np.int8)[new_to_sort]\n",
    "sort_features = features.astype(np.float32)[new_to_sort]\n",
    "\n",
    "print 'names equal:', np.array_equal(sort_all_names, sort_keys)\n",
    "print 'labels equal:', np.array_equal(sort_all_labels, sort_labels)\n",
    "print 'features equal:', np.array_equal(sort_imp_new_array, sort_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([19]), array([2142]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(sort_imp_new_array != sort_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02454611] [ 0.02454611] are equal:? [False]\n"
     ]
    }
   ],
   "source": [
    "original = sort_features[np.where(sort_imp_new_array != sort_features)] \n",
    "new = sort_imp_new_array[np.where(sort_imp_new_array != sort_features)]\n",
    "print original , new, 'are equal:?', original == new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the two arrays seems to be a rounding error. \n",
    "\n",
    "Will save the `vector_dict` and its keys (the keys as `vector_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "HOME_PATH = os.path.expanduser('~')\n",
    "DATA_PATH = os.path.join(HOME_PATH, 'Desktop', 'raw_data', 'ml')\n",
    "vector_path = os.path.join(DATA_PATH, 'vector_dict.pkl')\n",
    "name_path = os.path.join(DATA_PATH, 'vector_names.pkl')\n",
    "\n",
    "with open(vector_path, 'wb') as f:\n",
    "    pickle.dump(vector_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(name_path, 'wb') as f:\n",
    "    pickle.dump(sel_vocab_list, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
